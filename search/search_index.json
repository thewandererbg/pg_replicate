{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ETL Documentation","text":"<p>Build real-time Postgres replication applications in Rust</p> <p>ETL is a Rust framework by Supabase that enables you to build high-performance, real-time data replication applications for Postgres. Whether you're creating ETL pipelines, implementing CDC (Change Data Capture), or building custom data synchronization solutions, ETL provides the building blocks you need.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your path based on your needs:</p>"},{"location":"#new-to-etl","title":"New to ETL?","text":"<p>Start with our Tutorials to learn ETL through hands-on examples:</p> <ul> <li>Build your first ETL pipeline - Complete beginner's guide (15 minutes)</li> <li>Build custom stores and destinations - Advanced patterns (30 minutes)</li> </ul>"},{"location":"#ready-to-solve-specific-problems","title":"Ready to solve specific problems?","text":"<p>Jump to our How-To Guides for practical solutions:</p> <ul> <li>Configure Postgres for replication</li> <li>More guides coming soon</li> </ul>"},{"location":"#want-to-understand-the-bigger-picture","title":"Want to understand the bigger picture?","text":"<p>Read our Explanations for deeper insights:</p> <ul> <li>ETL architecture overview</li> <li>More explanations coming soon</li> </ul>"},{"location":"#core-concepts","title":"Core Concepts","text":"<p>Postgres Logical Replication streams data changes from Postgres databases in real-time using the Write-Ahead Log (WAL). ETL builds on this foundation to provide:</p> <ul> <li>\ud83d\ude80 Real-time replication - Stream changes as they happen</li> <li>\ud83d\udd04 Multiple destinations - BigQuery and more coming soon</li> <li>\ud83d\udee1\ufe0f Fault tolerance - Built-in error handling and recovery</li> <li>\u26a1 High performance - Efficient batching and parallel processing</li> <li>\ud83d\udd27 Extensible - Plugin architecture for custom destinations</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>use etl::{\n    config::{BatchConfig, PgConnectionConfig, PipelineConfig, TlsConfig},\n    destination::memory::MemoryDestination,\n    pipeline::Pipeline,\n    store::both::memory::MemoryStore,\n};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // Configure Postgres connection\n    let pg_config = PgConnectionConfig {\n        host: \"localhost\".to_string(),\n        port: 5432,\n        name: \"mydb\".to_string(),\n        username: \"postgres\".to_string(),\n        password: Some(\"password\".to_string().into()),\n        tls: TlsConfig { enabled: false, trusted_root_certs: String::new() },\n    };\n\n    // Create memory-based store and destination for testing\n    let store = MemoryStore::new();\n    let destination = MemoryDestination::new();\n\n    // Configure the pipeline\n    let config = PipelineConfig {\n        id: 1,\n        publication_name: \"my_publication\".to_string(),\n        pg_connection: pg_config,\n        batch: BatchConfig { max_size: 1000, max_fill_ms: 5000 },\n        table_error_retry_delay_ms: 10000,\n        max_table_sync_workers: 4,\n    };\n\n    // Create and start the pipeline\n    let mut pipeline = Pipeline::new(config, store, destination);\n    pipeline.start().await?;\n\n    // Pipeline will run until stopped\n    pipeline.wait().await?;\n\n    Ok(())\n}\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>First time using ETL? \u2192 Start with Build your first pipeline</li> <li>Need Postgres setup help? \u2192 Check Configure Postgres for Replication</li> <li>Need technical details? \u2192 Check the Reference</li> <li>Want to understand the architecture? \u2192 Read ETL Architecture</li> </ul>"},{"location":"explanation/","title":"Explanations","text":"<p>Deep dives into ETL concepts, architecture, and design decisions</p> <p>Explanations help you build mental models of how ETL works and why it's designed the way it is. These topics provide background knowledge, compare alternatives, and explore the reasoning behind key architectural choices.</p>"},{"location":"explanation/#core-concepts","title":"Core Concepts","text":""},{"location":"explanation/#etl-architecture-overview","title":"ETL Architecture Overview","text":"<p>The big picture of how ETL components work together</p> <p>Understand the relationship between pipelines, destinations, stores, and the Postgres replication protocol. Learn how data flows through the system and where extension points exist.</p> <p>Topics covered: Component architecture, data flow, extension patterns, scalability considerations.</p>"},{"location":"explanation/#reading-guide","title":"Reading Guide","text":"<p>New to ETL? Start with the ETL Architecture to understand how the system works.</p> <p>Planning a production deployment? Read Architecture to understand system behavior.</p> <p>Building extensions? Check out the Custom Implementations Tutorial.</p>"},{"location":"explanation/#next-steps","title":"Next Steps","text":"<p>After building a conceptual understanding:</p> <ul> <li>Start building \u2192 Tutorials</li> <li>Solve specific problems \u2192 How-To Guides</li> <li>Look up technical details \u2192 Reference</li> </ul>"},{"location":"explanation/#contributing-to-explanations","title":"Contributing to Explanations","text":"<p>Found gaps in these explanations? See something that could be clearer? Open an issue or contribute improvements to help other users build better mental models of ETL.</p>"},{"location":"explanation/architecture/","title":"ETL Architecture Overview","text":"<p>Understanding how ETL components work together to replicate data from Postgres</p> <p>ETL's architecture centers around five core abstractions that work together to provide reliable, high-performance data replication: <code>Pipeline</code>, <code>Destination</code>, <code>SchemaStore</code>, <code>StateStore</code>, and <code>CleanupStore</code>. This document explains how these components interact and coordinate data flow from Postgres logical replication to target systems.</p> <p>A diagram of the overall architecture is shown below:</p> <pre><code>flowchart LR\n    subgraph Postgres\n        A[\"WAL Stream&lt;br&gt;Publications&lt;br&gt;Replication Slots\"]\n    end\n\n    subgraph ETL_Pipeline[ETL Pipeline]\n        subgraph ApplyWorker[Apply Worker]\n            B1[\"CDC Events Processing and Tables Synchronization\"]\n        end\n\n        subgraph TableSyncWorkers[Table Sync Workers]\n            B2[\"Table 1 Sync + CDC\"]\n            B3[\"Table 2 Sync + CDC\"]\n            B4[\"Table N Sync + CDC\"]\n        end\n    end\n\n    subgraph Destination[Destination]\n        Dest[\"BigQuery&lt;br&gt;Custom API&lt;br&gt;Memory\"]\n    end\n\n    subgraph Store[Store]\n        subgraph StateStore[State Store]\n            D1[\"Memory&lt;br&gt;Postgres\"]\n        end\n\n        subgraph SchemaStore[Schema Store]\n            D2[\"Memory&lt;br&gt;Postgres\"]\n        end\n\n        subgraph CleanupStore[Cleanup Store]\n            D3[\"Memory&lt;br&gt;Postgres\"]\n        end\n    end\n\n    A --&gt; ApplyWorker\n    ApplyWorker --&gt; TableSyncWorkers\n\n    ApplyWorker --&gt; Destination\n    TableSyncWorkers --&gt; Destination\n\n    ApplyWorker --&gt; Store\n    TableSyncWorkers --&gt; Store</code></pre>"},{"location":"explanation/architecture/#core-abstractions","title":"Core Abstractions","text":""},{"location":"explanation/architecture/#pipeline","title":"Pipeline","text":"<p>The Pipeline is ETL's central component that orchestrates all replication activity. It manages worker lifecycles, coordinates data flow, and handles error recovery.</p> <p>Key responsibilities:</p> <ul> <li>Initializes the state of the pipeline</li> <li>Spawns the apply worker and table sync workers pool</li> <li>Tracks workers handles to wait for their termination</li> <li>Exposes the shutdown mechanism for gracefully terminating the pipeline</li> </ul>"},{"location":"explanation/architecture/#destination","title":"Destination","text":"<p>The <code>Destination</code> trait defines how replicated data is delivered to target systems:</p> <pre><code>pub trait Destination {\n    fn truncate_table(&amp;self, table_id: TableId) -&gt; impl Future&lt;Output = EtlResult&lt;()&gt;&gt; + Send;\n\n    fn write_table_rows(\n        &amp;self,\n        table_id: TableId,\n        table_rows: Vec&lt;TableRow&gt;,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;()&gt;&gt; + Send;\n\n    fn write_events(&amp;self, events: Vec&lt;Event&gt;) -&gt; impl Future&lt;Output = EtlResult&lt;()&gt;&gt; + Send;\n}\n</code></pre> <p>The trait provides three operations:</p> <ul> <li><code>truncate_table</code>: clears destination tables before bulk loading.</li> <li><code>write_table_rows</code>: handles bulk data insertion during initial synchronization.</li> <li><code>write_events</code>: processes streaming replication changes.</li> </ul>"},{"location":"explanation/architecture/#schemastore","title":"SchemaStore","text":"<p>The <code>SchemaStore</code> trait manages table schema information:</p> <pre><code>pub trait SchemaStore {\n    fn get_table_schema(\n        &amp;self,\n        table_id: &amp;TableId,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;Option&lt;Arc&lt;TableSchema&gt;&gt;&gt;&gt; + Send;\n\n    fn get_table_schemas(&amp;self) -&gt; impl Future&lt;Output = EtlResult&lt;Vec&lt;Arc&lt;TableSchema&gt;&gt;&gt;&gt; + Send;\n\n    fn load_table_schemas(&amp;self) -&gt; impl Future&lt;Output = EtlResult&lt;usize&gt;&gt; + Send;\n\n    fn store_table_schema(\n        &amp;self,\n        table_schema: TableSchema,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;()&gt;&gt; + Send;\n}\n</code></pre> <p>The store follows a cache-first pattern: <code>load_table_schemas</code> populates an in-memory cache at startup, while <code>get_table_schemas</code> methods read only from cache for performance. <code>store_table_schema</code> implements dual-write to both persistent storage and cache.</p>"},{"location":"explanation/architecture/#statestore","title":"StateStore","text":"<p>The <code>StateStore</code> trait manages replication state and table mappings:</p> <pre><code>pub trait StateStore {\n    fn get_table_replication_state(\n        &amp;self,\n        table_id: TableId,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;Option&lt;TableReplicationPhase&gt;&gt;&gt; + Send;\n\n    fn get_table_replication_states(\n        &amp;self,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;HashMap&lt;TableId, TableReplicationPhase&gt;&gt;&gt; + Send;\n\n    fn load_table_replication_states(&amp;self) -&gt; impl Future&lt;Output = EtlResult&lt;usize&gt;&gt; + Send;\n\n    fn update_table_replication_state(\n        &amp;self,\n        table_id: TableId,\n        state: TableReplicationPhase,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;()&gt;&gt; + Send;\n\n    fn rollback_table_replication_state(\n        &amp;self,\n        table_id: TableId,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;TableReplicationPhase&gt;&gt; + Send;\n\n    fn get_table_mapping(\n        &amp;self,\n        source_table_id: &amp;TableId,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;Option&lt;String&gt;&gt;&gt; + Send;\n\n    fn get_table_mappings(\n        &amp;self,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;HashMap&lt;TableId, String&gt;&gt;&gt; + Send;\n\n    fn load_table_mappings(&amp;self) -&gt; impl Future&lt;Output = EtlResult&lt;usize&gt;&gt; + Send;\n\n    fn store_table_mapping(\n        &amp;self,\n        source_table_id: TableId,\n        destination_table_id: String,\n    ) -&gt; impl Future&lt;Output = EtlResult&lt;()&gt;&gt; + Send;\n}\n</code></pre> <p>Like <code>SchemaStore</code>, <code>StateStore</code> uses cache-first reads with <code>load_*</code> methods for startup population and dual-write patterns for updates.</p> <p>The store tracks both replication progress through <code>TableReplicationPhase</code> and source-to-destination table name mappings.</p>"},{"location":"explanation/architecture/#cleanupstore","title":"CleanupStore","text":"<p>The <code>CleanupStore</code> trait provides atomic, table-scoped maintenance operations that affect both schema and state storage. The pipeline calls these primitives when tables are removed from a publication.</p> <pre><code>pub trait CleanupStore {\n    /// Deletes all stored state for `table_id` for the current pipeline.\n    ///\n    /// Removes replication state (including history), table schemas, and\n    /// table mappings. This must NOT drop or modify the actual destination table.\n    ///\n    /// Intended for use when a table is removed from the publication.\n    fn cleanup_table_state(&amp;self, table_id: TableId) -&gt; impl Future&lt;Output = EtlResult&lt;()&gt;&gt; + Send;\n}\n</code></pre> <p>Implementations must ensure the operation is consistent across in-memory caches and persistent storage, and must be idempotent. Cleanup only removes ETL-maintained metadata and state; it never touches destination tables.</p>"},{"location":"explanation/architecture/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"explanation/architecture/#worker-coordination","title":"Worker Coordination","text":"<p>ETL's data flow is orchestrated through two types of workers.</p>"},{"location":"explanation/architecture/#apply-worker","title":"Apply Worker","text":"<ul> <li>Processes Postgres logical replication stream</li> <li>Spawns table sync workers when new table are discovered</li> <li>Coordinates with table sync workers through shared state</li> <li>Handles final event processing for tables in <code>Ready</code> state</li> </ul>"},{"location":"explanation/architecture/#table-sync-workers","title":"Table Sync Workers","text":"<ul> <li>Perform bulk copying of existing table data</li> <li>Coordinate handoff to apply worker when synchronization completes</li> <li>Multiple table sync workers run in parallel, limited by configured semaphore to bound number of connections</li> </ul>"},{"location":"explanation/architecture/#worker-startup-sequence","title":"Worker Startup Sequence","text":"<p>The Pipeline follows this startup sequence:</p> <ol> <li>Pipeline Initialization: Establishes Postgres connection and loads cached state</li> <li>Apply Worker Launch: Creates and starts the primary apply worker first</li> <li>Table Discovery: Apply worker identifies tables requiring synchronization</li> <li>Table Sync Spawning: Apply worker spawns table sync workers for tables in <code>Init</code> state</li> <li>Coordination: Workers communicate through shared state store</li> <li>Streaming: Apply worker starts streaming replication events of table in <code>Ready</code> state and at every commit point    checks for new tables to synchronize</li> </ol> <p>The apply worker always starts first because it coordinates the overall replication process and spawns table sync workers on demand.</p>"},{"location":"explanation/architecture/#table-replication-phases","title":"Table Replication Phases","text":"<p>Each table progresses through distinct phases during replication:</p> <pre><code>pub enum TableReplicationPhase {\n    Init,\n    DataSync,\n    FinishedCopy,\n    SyncWait,\n    Catchup { lsn: PgLsn },\n    SyncDone { lsn: PgLsn },\n    Ready,\n    Errored { reason: String, solution: Option&lt;String&gt;, retry_policy: RetryPolicy },\n}\n</code></pre> <p>Phase Ownership and Transitions:</p> <ul> <li>Init: The table is discovered and ready to be copied</li> <li>DataSync: The table copy has started and is in progress</li> <li>FinishedCopy: The table has been fully copied and is ready to start CDC streaming</li> <li>SyncWait: The table is ready to start CDC streaming and is waiting for the apply worker to tell which LSN to catchup</li> <li>Catchup: The table is catching up to the the LSN specified by the apply worker</li> <li>SyncDone: The table has caught up to the LSN specified by the apply worker</li> <li>Ready: The table is now copied and caught up with the apply worker, now all events are processed by the apply worker for this table</li> <li>Errored: The table has encountered an error and is excluded from replication until a rollback is performed</li> </ul>"},{"location":"explanation/architecture/#next-steps","title":"Next Steps","text":"<p>Now that you understand ETL's architecture:</p> <ul> <li>Build your first pipeline \u2192 First Pipeline Tutorial</li> <li>Implement custom components \u2192 Custom Stores and Destinations</li> <li>Configure Postgres properly \u2192 Configure Postgres for Replication</li> </ul>"},{"location":"explanation/architecture/#see-also","title":"See Also","text":"<ul> <li>Build Your First ETL Pipeline - Hands-on tutorial using these components</li> <li>Custom Stores and Destinations - Implement your own stores and destinations</li> <li>API Reference - Complete trait documentation</li> <li>Configure Postgres for Replication - Set up the source database</li> </ul>"},{"location":"how-to/","title":"How-To Guides","text":"<p>Practical solutions for common ETL tasks</p> <p>How-to guides provide step-by-step instructions for accomplishing specific goals when working with ETL. Each guide assumes you're already familiar with ETL basics and focuses on the task at hand.</p>"},{"location":"how-to/#database-configuration","title":"Database Configuration","text":""},{"location":"how-to/#configure-postgres-for-replication","title":"Configure Postgres for Replication","text":"<p>Set up Postgres with the correct settings, and publications for ETL pipelines.</p> <p>When to use: Setting up a new Postgres source for replication.</p>"},{"location":"how-to/#next-steps","title":"Next Steps","text":"<p>After solving your immediate problem:</p> <ul> <li>Learn more concepts \u2192 Explanations</li> <li>Look up technical details \u2192 Reference</li> <li>Build foundational knowledge \u2192 Tutorials</li> </ul>"},{"location":"how-to/#need-help","title":"Need Help?","text":"<p>If these guides don't cover your specific situation:</p> <ol> <li>Check the Postgres configuration guide</li> <li>Search existing GitHub issues</li> <li>Open a new issue with details about your use case</li> </ol>"},{"location":"how-to/configure-postgres/","title":"Configure Postgres for Replication","text":"<p>Set up Postgres with the correct permissions and settings for ETL logical replication</p> <p>This guide covers the essential Postgres concepts and configuration needed for logical replication with ETL.</p>"},{"location":"how-to/configure-postgres/#prerequisites","title":"Prerequisites","text":"<ul> <li>Postgres 10 or later</li> <li>Superuser access to the Postgres server</li> <li>Ability to restart Postgres server (for configuration changes)</li> </ul>"},{"location":"how-to/configure-postgres/#understanding-wal-logical","title":"Understanding WAL Logical","text":"<p>Postgres's Write-Ahead Log (WAL) is the foundation of logical replication. When <code>wal_level = logical</code>, Postgres:</p> <ul> <li>Records detailed information about data changes (not just physical changes)</li> <li>Includes enough metadata to reconstruct logical changes</li> <li>Allows external tools to decode and stream these changes</li> </ul> <pre><code># Enable logical replication in Postgres.conf\nwal_level = logical\n</code></pre> <p>Restart Postgres after changing this setting:</p> <pre><code>sudo systemctl restart Postgres\n</code></pre>"},{"location":"how-to/configure-postgres/#replication-slots","title":"Replication Slots","text":"<p>Replication slots ensure that Postgres retains WAL data for logical replication consumers, even if they disconnect temporarily.</p>"},{"location":"how-to/configure-postgres/#what-are-replication-slots","title":"What are Replication Slots?","text":"<ul> <li>Persistent markers in Postgres that track replication progress</li> <li>Prevent WAL cleanup until the consumer catches up</li> <li>Guarantee data consistency across disconnections</li> </ul>"},{"location":"how-to/configure-postgres/#creating-replication-slots","title":"Creating Replication Slots","text":"<pre><code>-- Create a logical replication slot\nSELECT pg_create_logical_replication_slot('my_slot', 'pgoutput');\n</code></pre>"},{"location":"how-to/configure-postgres/#viewing-replication-slots","title":"Viewing Replication Slots","text":"<pre><code>-- See all replication slots\nSELECT slot_name, slot_type, active, restart_lsn\nFROM pg_replication_slots;\n</code></pre>"},{"location":"how-to/configure-postgres/#deleting-replication-slots","title":"Deleting Replication Slots","text":"<pre><code>-- Drop a replication slot when no longer needed\nSELECT pg_drop_replication_slot('my_slot');\n</code></pre> <p>Warning: Only delete slots when you're sure they're not in use. Deleting an active slot can cause data loss.</p>"},{"location":"how-to/configure-postgres/#max-replication-slots","title":"Max Replication Slots","text":"<p>Controls how many replication slots Postgres can maintain simultaneously.</p> <pre><code># Increase max replication slots (default is 10)\nmax_replication_slots = 20\n</code></pre> <p>ETL uses a single replication slot for its main apply worker. However, additional slots may be created for parallel table copies when the pipeline is initialized or when a new table is added to the publication. The <code>max_table_sync_workers</code> parameter controls the number of these parallel copies, ensuring that the total replication slots used by ETL never exceed <code>max_table_sync_workers + 1</code>.</p> <p>When to increase:</p> <ul> <li>Running multiple ETL pipelines</li> <li>Development/testing with frequent slot creation</li> </ul>"},{"location":"how-to/configure-postgres/#wal-keep-size","title":"WAL Keep Size","text":"<p>Determines how much WAL data to retain on disk, providing a safety buffer for replication consumers.</p> <pre><code># Keep 1GB of WAL data (Postgres 13+)\nwal_keep_size = 1GB\n\n# For Postgres 12 and earlier, use:\n# wal_keep_segments = 256  # Each segment is typically 16MB\n</code></pre> <p>Purpose:</p> <ul> <li>Prevents WAL deletion when replication consumers fall behind</li> <li>Provides recovery time if ETL pipelines temporarily disconnect</li> <li>Balances disk usage with replication reliability</li> </ul>"},{"location":"how-to/configure-postgres/#publications","title":"Publications","text":"<p>Publications define which tables and operations to replicate.</p>"},{"location":"how-to/configure-postgres/#creating-publications","title":"Creating Publications","text":"<pre><code>-- Create publication for specific tables\nCREATE PUBLICATION my_publication FOR TABLE users, orders;\n\n-- Create publication for all tables (use with caution)\nCREATE PUBLICATION all_tables FOR ALL TABLES;\n\n-- Include only specific operations\nCREATE PUBLICATION inserts_only FOR TABLE users WITH (publish = 'insert');\n</code></pre>"},{"location":"how-to/configure-postgres/#managing-publications","title":"Managing Publications","text":"<pre><code>-- View existing publications\nSELECT * FROM pg_publication;\n\n-- See which tables are in a publication\nSELECT * FROM pg_publication_tables WHERE pubname = 'my_publication';\n\n-- Add tables to existing publication\nALTER PUBLICATION my_publication ADD TABLE products;\n\n-- Remove tables from publication\nALTER PUBLICATION my_publication DROP TABLE products;\n\n-- Drop publication\nDROP PUBLICATION my_publication;\n</code></pre>"},{"location":"how-to/configure-postgres/#complete-configuration-example","title":"Complete Configuration Example","text":"<p>Here's a minimal <code>Postgres.conf</code> setup:</p> <pre><code># Enable logical replication\nwal_level = logical\n\n# Increase replication capacity\nmax_replication_slots = 20\nmax_wal_senders = 20\n\n# Keep WAL data for safety\nwal_keep_size = 1GB  # Postgres 13+\n# wal_keep_segments = 64  # Postgres 12 and earlier\n</code></pre> <p>After editing the configuration:</p> <ol> <li>Restart Postgres</li> <li>Create your publication:    <pre><code>CREATE PUBLICATION etl_publication FOR TABLE your_table;\n</code></pre></li> <li>Verify the setup:    <pre><code>SHOW wal_level;\nSHOW max_replication_slots;\nSELECT * FROM pg_publication WHERE pubname = 'etl_publication';\n</code></pre></li> </ol>"},{"location":"how-to/configure-postgres/#next-steps","title":"Next Steps","text":"<ul> <li>Build your first pipeline \u2192 Build Your First ETL Pipeline</li> <li>Build custom implementations \u2192 Custom Stores and Destinations</li> </ul>"},{"location":"how-to/configure-postgres/#see-also","title":"See Also","text":"<ul> <li>Build Your First ETL Pipeline - Hands-on tutorial using these settings</li> <li>ETL Architecture - Understanding how ETL uses these settings</li> <li>API Reference - All available connection options</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Complete API documentation is available through Rust's built-in documentation system. We will publish comprehensive rustdoc documentation that covers all public APIs, traits, and configuration structures. Right now the docs are accessible via the code or by running: <pre><code>cargo doc --workspace --all-features --no-deps --open\n</code></pre></p>"},{"location":"reference/#see-also","title":"See Also","text":"<ul> <li>How-to guides - Task-oriented instructions</li> <li>Tutorials - Learning-oriented lessons  </li> <li>Explanations - Understanding-oriented discussions</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Learn ETL through guided, hands-on experiences</p> <p>Tutorials provide step-by-step learning paths that take you from zero knowledge to working applications. Each tutorial is designed to be completed successfully by following the exact steps provided.</p>"},{"location":"tutorials/#getting-started","title":"Getting Started","text":""},{"location":"tutorials/#build-your-first-etl-pipeline","title":"Build Your First ETL Pipeline","text":"<p>15 minutes \u2022 Beginner</p> <p>Create a complete ETL pipeline that replicates data from Postgres to a memory destination. You'll learn the core concepts of publications, replication slots, and pipeline configuration.</p> <p>What you'll build: A working pipeline that streams changes from a sample Postgres table to an in-memory destination.</p>"},{"location":"tutorials/#advanced-topics","title":"Advanced Topics","text":""},{"location":"tutorials/#build-custom-stores-and-destinations","title":"Build Custom Stores and Destinations","text":"<p>45 minutes \u2022 Advanced</p> <p>Implement production-ready custom stores and destinations. Learn ETL's design patterns, build persistent storage, implement cleanup primitives for safe table removal, and create HTTP-based destinations with retry logic.</p> <p>What you'll build: Custom in-memory store for state/schema storage with cleanup, and an HTTP destination.</p>"},{"location":"tutorials/#before-you-start","title":"Before You Start","text":"<p>Prerequisites for all tutorials:</p> <ul> <li>Rust installed (1.75 or later)</li> <li>Postgres server (local or remote)</li> <li>Basic familiarity with Rust and SQL</li> </ul> <p>What you'll need:</p> <ul> <li>A terminal/command line</li> <li>Your favorite text editor</li> <li>About 30-60 minutes total time</li> </ul>"},{"location":"tutorials/#next-steps","title":"Next Steps","text":"<p>After completing the tutorials:</p> <ul> <li>Solve specific problems \u2192 How-To Guides</li> <li>Understand the architecture \u2192 ETL Architecture</li> </ul>"},{"location":"tutorials/#need-help","title":"Need Help?","text":"<p>If you get stuck:</p> <ol> <li>Double-check the prerequisites</li> <li>Ensure your Postgres setup matches the requirements</li> <li>Check the Postgres configuration guide</li> <li>Open an issue with your specific problem</li> </ol>"},{"location":"tutorials/custom-implementations/","title":"Build Custom Stores and Destinations in 30 minutes","text":"<p>Learn ETL's extension patterns by implementing working custom components</p>"},{"location":"tutorials/custom-implementations/#what-youll-build","title":"What You'll Build","text":"<p>By the end of this tutorial, you'll have:</p> <ul> <li>A working custom in-memory store that logs all operations for debugging</li> <li>A custom HTTP destination that sends data with automatic retries</li> <li>A complete pipeline using your custom components that processes real data</li> </ul> <p>Time required: 30 minutes Prerequisites: Advanced Rust knowledge, running Postgres, basic HTTP knowledge</p>"},{"location":"tutorials/custom-implementations/#step-1-create-project-structure","title":"Step 1: Create Project Structure","text":"<p>Create a new Rust project for your custom ETL components:</p> <pre><code>cargo new etl-custom --lib\ncd etl-custom\n</code></pre> <p>Result: You should see <code>Created library 'etl-custom' package</code> output.</p>"},{"location":"tutorials/custom-implementations/#step-2-add-dependencies","title":"Step 2: Add Dependencies","text":"<p>Replace your <code>Cargo.toml</code> with the required dependencies:</p> <pre><code>[package]\nname = \"etl-custom\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[[bin]]\nname = \"main\"\npath = \"src/main.rs\"\n\n[dependencies]\netl = { git = \"https://github.com/supabase/etl\" }\ntokio = { version = \"1.0\", features = [\"full\"] }\nreqwest = { version = \"0.11\", features = [\"json\"] }\nserde_json = \"1.0\"\ntracing = \"0.1\"\ntracing-subscriber = \"0.3\"\n</code></pre> <p>Result: Running <code>cargo check</code> should download dependencies without errors.</p>"},{"location":"tutorials/custom-implementations/#step-3-create-custom-store-implementation","title":"Step 3: Create Custom Store Implementation","text":"<p>Create <code>src/custom_store.rs</code> with a dual-storage implementation and cleanup primitives:</p> <pre><code>use std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::Mutex;\nuse tracing::info;\n\nuse etl::error::EtlResult;\nuse etl::state::table::TableReplicationPhase;\nuse etl::store::schema::SchemaStore;\nuse etl::store::state::StateStore;\nuse etl::store::cleanup::CleanupStore;\nuse etl::types::{TableId, TableSchema};\n\n// Represents data stored in our in-memory cache for fast access\n#[derive(Debug, Clone)]\nstruct CachedEntry {\n    schema: Option&lt;Arc&lt;TableSchema&gt;&gt;,          // Table structure info\n    state: Option&lt;TableReplicationPhase&gt;,      // Current replication progress\n    mapping: Option&lt;String&gt;,                   // Source -&gt; destination table mapping\n}\n\n// Represents data as it would be stored persistently (e.g., in files/database)\n#[derive(Debug, Clone)]\nstruct PersistentEntry {\n    schema: Option&lt;TableSchema&gt;,               // Not Arc-wrapped in \"persistent\" storage\n    state: Option&lt;TableReplicationPhase&gt;,\n    mapping: Option&lt;String&gt;,\n}\n\n#[derive(Debug, Clone)]\npub struct CustomStore {\n    // Fast in-memory cache for all read operations - this is what ETL queries\n    cache: Arc&lt;Mutex&lt;HashMap&lt;TableId, CachedEntry&gt;&gt;&gt;,\n    // Simulated persistent storage - in real implementation this might be files/database\n    persistent: Arc&lt;Mutex&lt;HashMap&lt;TableId, PersistentEntry&gt;&gt;&gt;,\n}\n\nimpl CustomStore {\n    pub fn new() -&gt; Self {\n        info!(\"Creating custom store with dual-layer architecture (cache + persistent)\");\n        Self {\n            cache: Arc::new(Mutex::new(HashMap::new())),\n            persistent: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    // Helper to ensure we have a cache entry to work with - creates if missing\n    fn ensure_cache_slot&lt;'a&gt;(\n        cache: &amp;'a mut HashMap&lt;TableId, CachedEntry&gt;,\n        id: TableId,\n    ) -&gt; &amp;'a mut CachedEntry {\n        cache.entry(id).or_insert_with(|| {\n            // Initialize empty entry if this table hasn't been seen before\n            CachedEntry {\n                schema: None,\n                state: None,\n                mapping: None\n            }\n        })\n    }\n\n    // Helper to ensure we have a persistent entry to work with - creates if missing\n    fn ensure_persistent_slot&lt;'a&gt;(\n        persistent: &amp;'a mut HashMap&lt;TableId, PersistentEntry&gt;,\n        id: TableId,\n    ) -&gt; &amp;'a mut PersistentEntry {\n        persistent.entry(id).or_insert_with(|| {\n            // Initialize empty persistent entry if this table hasn't been seen before\n            PersistentEntry {\n                schema: None,\n                state: None,\n                mapping: None\n            }\n        })\n    }\n}\n\n// Implementation of ETL's SchemaStore trait - handles table structure information\nimpl SchemaStore for CustomStore {\n    // ETL calls this frequently during data processing - must be fast (cache-only)\n    async fn get_table_schema(&amp;self, table_id: &amp;TableId) -&gt; EtlResult&lt;Option&lt;Arc&lt;TableSchema&gt;&gt;&gt; {\n        let cache = self.cache.lock().await;\n        let result = cache.get(table_id).and_then(|e| e.schema.clone());\n        info!(\"Schema cache read for table {}: found={}\", table_id.0, result.is_some());\n        Ok(result)\n    }\n\n    // Return all cached schemas - used by ETL for bulk operations\n    async fn get_table_schemas(&amp;self) -&gt; EtlResult&lt;Vec&lt;Arc&lt;TableSchema&gt;&gt;&gt; {\n        let cache = self.cache.lock().await;\n        let schemas: Vec&lt;_&gt; = cache.values()\n            .filter_map(|e| e.schema.clone())  // Only include entries that have schemas\n            .collect();\n        info!(\"Retrieved {} schemas from cache\", schemas.len());\n        Ok(schemas)\n    }\n\n    // Called at startup - load persistent data into cache for fast access\n    async fn load_table_schemas(&amp;self) -&gt; EtlResult&lt;usize&gt; {\n        info!(\"Loading schemas from persistent storage into cache (startup phase)\");\n        let persistent = self.persistent.lock().await;\n        let mut cache = self.cache.lock().await;\n\n        let mut loaded = 0;\n        for (id, pentry) in persistent.iter() {\n            if let Some(schema) = &amp;pentry.schema {\n                // Move schema from persistent storage to cache, wrapping in Arc for sharing\n                let centry = Self::ensure_cache_slot(&amp;mut cache, *id);\n                centry.schema = Some(Arc::new(schema.clone()));\n                loaded += 1;\n            }\n        }\n        info!(\"Loaded {} schemas into cache from persistent storage\", loaded);\n        Ok(loaded)\n    }\n\n    // Store new schema - implements dual-write pattern (persistent first, then cache)\n    async fn store_table_schema(&amp;self, table_schema: TableSchema) -&gt; EtlResult&lt;()&gt; {\n        let id = table_schema.id;\n        info!(\"Storing schema for table {} using dual-write pattern\", id.0);\n\n        // First write to persistent storage (this would be a file/database in reality)\n        {\n            let mut persistent = self.persistent.lock().await;\n            let p = Self::ensure_persistent_slot(&amp;mut persistent, id);\n            p.schema = Some(table_schema.clone());\n        }\n        // Then update cache for immediate availability\n        {\n            let mut cache = self.cache.lock().await;\n            let c = Self::ensure_cache_slot(&amp;mut cache, id);\n            c.schema = Some(Arc::new(table_schema));\n        }\n        Ok(())\n    }\n}\n\n// Implementation of ETL's StateStore trait - handles replication progress tracking\nimpl StateStore for CustomStore {\n    // Get current replication state for a table - cache-only for speed\n    async fn get_table_replication_state(\n        &amp;self,\n        table_id: TableId,\n    ) -&gt; EtlResult&lt;Option&lt;TableReplicationPhase&gt;&gt; {\n        let cache = self.cache.lock().await;\n        let result = cache.get(&amp;table_id).and_then(|e| e.state.clone());\n        info!(\"State cache read for table {}: {:?}\", table_id.0, result);\n        Ok(result)\n    }\n\n    // Get all replication states - used by ETL to understand overall progress\n    async fn get_table_replication_states(\n        &amp;self,\n    ) -&gt; EtlResult&lt;HashMap&lt;TableId, TableReplicationPhase&gt;&gt; {\n        let cache = self.cache.lock().await;\n        let states: HashMap&lt;_, _&gt; = cache.iter()\n            .filter_map(|(id, e)| e.state.clone().map(|s| (*id, s)))  // Only include tables with state\n            .collect();\n        info!(\"Retrieved {} table states from cache\", states.len());\n        Ok(states)\n    }\n\n    // Load persistent states into cache at startup\n    async fn load_table_replication_states(&amp;self) -&gt; EtlResult&lt;usize&gt; {\n        info!(\"Loading replication states from persistent storage into cache\");\n        let persistent = self.persistent.lock().await;\n        let mut cache = self.cache.lock().await;\n\n        let mut loaded = 0;\n        for (id, pentry) in persistent.iter() {\n            if let Some(state) = pentry.state.clone() {\n                // Move state from persistent to cache\n                let centry = Self::ensure_cache_slot(&amp;mut cache, *id);\n                centry.state = Some(state);\n                loaded += 1;\n            }\n        }\n        info!(\"Loaded {} replication states into cache\", loaded);\n        Ok(loaded)\n    }\n\n    // Update replication state - critical for tracking progress, uses dual-write\n    async fn update_table_replication_state(\n        &amp;self,\n        table_id: TableId,\n        state: TableReplicationPhase,\n    ) -&gt; EtlResult&lt;()&gt; {\n        info!(\"Updating replication state for table {} to {:?} (dual-write)\", table_id.0, state);\n\n        // First persist the state (ensures durability)\n        {\n            let mut persistent = self.persistent.lock().await;\n            let p = Self::ensure_persistent_slot(&amp;mut persistent, table_id);\n            p.state = Some(state.clone());\n        }\n        // Then update cache (ensures immediate availability)\n        {\n            let mut cache = self.cache.lock().await;\n            let c = Self::ensure_cache_slot(&amp;mut cache, table_id);\n            c.state = Some(state);\n        }\n        Ok(())\n    }\n\n    // Rollback state to previous version - not implemented in this simple example\n    async fn rollback_table_replication_state(\n        &amp;self,\n        _table_id: TableId,\n    ) -&gt; EtlResult&lt;TableReplicationPhase&gt; {\n        // In a real implementation, you'd maintain state history and rollback to previous version\n        todo!(\"Implement state history tracking for rollback\")\n    }\n\n    // Get table name mapping from source to destination\n    async fn get_table_mapping(&amp;self, source_table_id: &amp;TableId) -&gt; EtlResult&lt;Option&lt;String&gt;&gt; {\n        let cache = self.cache.lock().await;\n        let mapping = cache.get(source_table_id).and_then(|e| e.mapping.clone());\n        info!(\"Mapping lookup for table {}: {:?}\", source_table_id.0, mapping);\n        Ok(mapping)\n    }\n\n    // Get all table mappings - used when ETL needs to understand all table relationships\n    async fn get_table_mappings(&amp;self) -&gt; EtlResult&lt;HashMap&lt;TableId, String&gt;&gt; {\n        let cache = self.cache.lock().await;\n        let mappings: HashMap&lt;_, _&gt; = cache.iter()\n            .filter_map(|(id, e)| e.mapping.clone().map(|m| (*id, m)))  // Only include mapped tables\n            .collect();\n        info!(\"Retrieved {} table mappings from cache\", mappings.len());\n        Ok(mappings)\n    }\n\n    // Load persistent mappings into cache at startup\n    async fn load_table_mappings(&amp;self) -&gt; EtlResult&lt;usize&gt; {\n        info!(\"Loading table mappings from persistent storage into cache\");\n        let persistent = self.persistent.lock().await;\n        let mut cache = self.cache.lock().await;\n\n        let mut loaded = 0;\n        for (id, pentry) in persistent.iter() {\n            if let Some(m) = &amp;pentry.mapping {\n                // Load mapping into cache\n                let centry = Self::ensure_cache_slot(&amp;mut cache, *id);\n                centry.mapping = Some(m.clone());\n                loaded += 1;\n            }\n        }\n        info!(\"Loaded {} table mappings into cache\", loaded);\n        Ok(loaded)\n    }\n\n    // Store a new table mapping (source table -&gt; destination table name)\n    async fn store_table_mapping(\n        &amp;self,\n        source_table_id: TableId,\n        destination_table_id: String,\n    ) -&gt; EtlResult&lt;()&gt; {\n        info!(\n            \"Storing table mapping: {} -&gt; {} (dual-write)\",\n            source_table_id.0, destination_table_id\n        );\n\n        // First persist the mapping\n        {\n            let mut persistent = self.persistent.lock().await;\n            let p = Self::ensure_persistent_slot(&amp;mut persistent, source_table_id);\n            p.mapping = Some(destination_table_id.clone());\n        }\n        // Then update cache\n        {\n            let mut cache = self.cache.lock().await;\n            let c = Self::ensure_cache_slot(&amp;mut cache, source_table_id);\n            c.mapping = Some(destination_table_id);\n        }\n        Ok(())\n    }\n}\n\n// Cleanup primitives spanning both schema and state storage\nimpl CleanupStore for CustomStore {\n    // Delete everything ETL tracks for a specific table in a consistent, idempotent way\n    async fn cleanup_table_state(&amp;self, table_id: TableId) -&gt; EtlResult&lt;()&gt; {\n        {\n            // Remove from persistent storage first\n            let mut persistent = self.persistent.lock().await;\n            persistent.remove(&amp;table_id);\n        }\n\n        {\n            // Then clear the cache to maintain consistency\n            let mut cache = self.cache.lock().await;\n            cache.remove(&amp;table_id);\n        }\n\n        Ok(())\n    }\n}\n</code></pre> <p>Result: Your file should compile without errors when you run <code>cargo check</code>.</p>"},{"location":"tutorials/custom-implementations/#step-4-create-http-destination-implementation","title":"Step 4: Create HTTP Destination Implementation","text":"<p>Create <code>src/http_destination.rs</code> with retry logic and proper error handling:</p> <pre><code>use reqwest::{Client, Method};\nuse serde_json::{Value, json};\nuse std::time::Duration;\nuse tracing::{info, warn};\n\nuse etl::destination::Destination;\nuse etl::error::{ErrorKind, EtlError, EtlResult};\nuse etl::types::{Event, TableId, TableRow};\nuse etl::{bail, etl_error};\n\n// Configuration constants for retry behavior\nconst MAX_RETRIES: usize = 3;      // Try up to 3 times before giving up\nconst BASE_BACKOFF_MS: u64 = 500;  // Start with 500ms delay, then exponential backoff\n\npub struct HttpDestination {\n    client: Client,        // HTTP client for making requests\n    base_url: String,      // Base URL for the destination API (e.g., \"https://api.example.com\")\n}\n\nimpl HttpDestination {\n    /// Create a new HTTP destination that will send data to the specified base URL\n    pub fn new(base_url: String) -&gt; EtlResult&lt;Self&gt; {\n        // Configure HTTP client with reasonable timeout\n        let client = Client::builder()\n            .timeout(Duration::from_secs(10))  // 10 second timeout for each request\n            .build()\n            .map_err(|e| etl_error!(ErrorKind::Unknown, \"Failed to create HTTP client\", e))?;\n\n        info!(\"Created HTTP destination pointing to: {}\", base_url);\n        Ok(Self { client, base_url })\n    }\n\n    /// Helper to construct full URLs by combining base URL with endpoint paths\n    fn url(&amp;self, path: &amp;str) -&gt; String {\n        format!(\n            \"{}/{}\",\n            self.base_url.trim_end_matches('/'),    // Remove trailing slash from base\n            path.trim_start_matches('/')           // Remove leading slash from path\n        )\n    }\n\n    /// Generic HTTP sender with automatic retry logic and exponential backoff\n    /// This handles all the complex retry logic so individual methods can focus on data formatting\n    async fn send_json(&amp;self, method: Method, path: &amp;str, body: Option&lt;&amp;Value&gt;) -&gt; EtlResult&lt;()&gt; {\n        let url = self.url(path);\n        info!(\"Attempting HTTP {} to {}\", method, url);\n\n        // Retry loop with exponential backoff\n        for attempt in 0..MAX_RETRIES {\n            // Build the request\n            let mut req = self.client.request(method.clone(), &amp;url);\n            if let Some(b) = body {\n                req = req.json(b);  // Add JSON body if provided\n            }\n\n            // Send the request and handle response\n            match req.send().await {\n                // Success case - 2xx status codes\n                Ok(resp) if resp.status().is_success() =&gt; {\n                    info!(\n                        \"HTTP {} {} succeeded on attempt {}/{}\",\n                        method, url, attempt + 1, MAX_RETRIES\n                    );\n                    return Ok(());\n                }\n                // HTTP error response (4xx/5xx)\n                Ok(resp) =&gt; {\n                    let status = resp.status();\n                    warn!(\n                        \"HTTP {} {} returned status {}, attempt {}/{}\",\n                        method, url, status, attempt + 1, MAX_RETRIES\n                    );\n\n                    // Don't retry client errors (4xx) - they won't succeed on retry\n                    if status.is_client_error() {\n                        bail!(\n                            ErrorKind::Unknown,\n                            \"HTTP client error - not retrying\",\n                            format!(\"Status: {}\", status)\n                        );\n                    }\n                    // Server errors (5xx) will be retried\n                }\n                // Network/connection errors - these are worth retrying\n                Err(e) =&gt; {\n                    warn!(\n                        \"HTTP {} {} network error on attempt {}/{}: {}\",\n                        method, url, attempt + 1, MAX_RETRIES, e\n                    );\n                }\n            }\n\n            // If this wasn't the last attempt, wait before retrying\n            if attempt + 1 &lt; MAX_RETRIES {\n                // Exponential backoff: 500ms, 1s, 2s (attempt 0, 1, 2)\n                let delay = Duration::from_millis(BASE_BACKOFF_MS * 2u64.pow(attempt as u32));\n                info!(\"Waiting {:?} before retry\", delay);\n                tokio::time::sleep(delay).await;\n            }\n        }\n\n        // All retries failed\n        bail!(\n            ErrorKind::Unknown,\n            \"HTTP request failed after all retries\",\n            format!(\"Exhausted {} attempts to {}\", MAX_RETRIES, url)\n        )\n    }\n}\n\n// Implementation of ETL's Destination trait - this is what ETL calls to send data\nimpl Destination for HttpDestination {\n    /// Called when ETL needs to clear all data from a table (e.g., during full refresh)\n    async fn truncate_table(&amp;self, table_id: TableId) -&gt; EtlResult&lt;()&gt; {\n        info!(\"Truncating destination table: {}\", table_id.0);\n\n        // Send DELETE request to truncate endpoint\n        self.send_json(\n            Method::DELETE,\n            &amp;format!(\"tables/{}/truncate\", table_id.0),  // e.g., \"tables/users/truncate\"\n            None,  // No body needed for truncate\n        ).await\n    }\n\n    /// Called when ETL has a batch of rows to send to the destination\n    /// This is the main data flow method - gets called frequently during replication\n    async fn write_table_rows(\n        &amp;self,\n        table_id: TableId,\n        table_rows: Vec&lt;TableRow&gt;,\n    ) -&gt; EtlResult&lt;()&gt; {\n        // Skip empty batches - no work to do\n        if table_rows.is_empty() {\n            info!(\"Skipping empty batch for table {}\", table_id.0);\n            return Ok(());\n        }\n\n        info!(\n            \"Sending {} rows to destination table {}\",\n            table_rows.len(),\n            table_id.0\n        );\n\n        // Convert ETL's internal row format to JSON that our API expects\n        // In a real implementation, you'd format this according to your destination's schema\n        let rows_json: Vec&lt;_&gt; = table_rows\n            .iter()\n            .map(|row| {\n                json!({\n                    \"values\": row.values.iter()\n                        .map(|v| format!(\"{:?}\", v))  // Simple string conversion for demo\n                        .collect::&lt;Vec&lt;_&gt;&gt;()\n                })\n            })\n            .collect();\n\n        // Create the JSON payload our API expects\n        let payload = json!({\n            \"table_id\": table_id.0,\n            \"rows\": rows_json\n        });\n\n        // Send POST request with the row data\n        self.send_json(\n            Method::POST,\n            &amp;format!(\"tables/{}/rows\", table_id.0),  // e.g., \"tables/users/rows\"\n            Some(&amp;payload),\n        ).await\n    }\n\n    /// Called when ETL has replication events to send (e.g., transaction markers)\n    /// These are metadata events about the replication process itself\n    async fn write_events(&amp;self, events: Vec&lt;Event&gt;) -&gt; EtlResult&lt;()&gt; {\n        // Skip if no events to process\n        if events.is_empty() {\n            return Ok(());\n        }\n\n        info!(\"Sending {} replication events to destination\", events.len());\n\n        // Convert events to JSON format\n        let events_json: Vec&lt;_&gt; = events\n            .iter()\n            .map(|event| {\n                json!({\n                    \"event_type\": format!(\"{:?}\", event),  // Convert event to string for demo\n                })\n            })\n            .collect();\n\n        let payload = json!({ \"events\": events_json });\n\n        // Send events to generic events endpoint\n        self.send_json(Method::POST, \"events\", Some(&amp;payload)).await\n    }\n}\n</code></pre> <p>Result: Run <code>cargo check</code> again - it should compile successfully with both your store and destination implementations.</p>"},{"location":"tutorials/custom-implementations/#step-5-create-working-pipeline-example","title":"Step 5: Create Working Pipeline Example","text":"<p>Create <code>src/main.rs</code> that demonstrates your custom components in action:</p> <pre><code>mod custom_store;\nmod http_destination;\n\nuse custom_store::CustomStore;\nuse http_destination::HttpDestination;\nuse etl::config::{BatchConfig, PgConnectionConfig, PipelineConfig, TlsConfig};\nuse etl::pipeline::Pipeline;\nuse tracing::{info, Level};\nuse std::time::Duration;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // Initialize logging so we can see what our custom components are doing\n    tracing_subscriber::fmt()\n        .with_max_level(Level::INFO)\n        .init();\n\n    info!(\"=== Starting ETL Pipeline with Custom Components ===\");\n\n    // Step 1: Create our custom store\n    // This will handle both schema storage and replication state tracking\n    info!(\"Creating custom dual-layer store (cache + persistent simulation)\");\n    let store = CustomStore::new();\n\n    // Step 2: Create our custom HTTP destination\n    // Using httpbin.org which echoes back what we send - perfect for testing\n    info!(\"Creating HTTP destination with retry logic\");\n    let destination = HttpDestination::new(\n        \"https://httpbin.org/post\".to_string()  // This endpoint will show us what we sent\n    )?;\n\n    // Step 3: Configure the Postgres connection\n    // Update these values to match your local Postgres setup\n    let pipeline_config = PipelineConfig {\n        id: 1,  // Unique pipeline identifier\n        publication_name: \"etl_demo_pub\".to_string(),  // Postgres publication name\n\n        // Postgres connection details - CHANGE THESE to match your setup\n        pg_connection: PgConnectionConfig {\n            host: \"localhost\".to_string(),\n            port: 5432,\n            name: \"postgres\".to_string(),          // Database name\n            username: \"postgres\".to_string(),      // Database user\n            password: Some(\"postgres\".to_string().into()),  // Update with your password\n            tls: TlsConfig {\n                enabled: false,  // Disable TLS for local development\n                trusted_root_certs: String::new()\n            },\n        },\n\n        // Batching configuration - controls how ETL groups data for efficiency\n        batch: BatchConfig {\n            max_size: 100,      // Send data when we have 100 rows\n            max_fill_ms: 5000   // Or send data every 5 seconds, whichever comes first\n        },\n\n        // Error handling configuration\n        table_error_retry_delay_ms: 10000,  // Wait 10s before retrying failed tables\n        max_table_sync_workers: 2,          // Use 2 workers for parallel table syncing\n    };\n\n    // Step 4: Create the pipeline with our custom components\n    // This combines your custom store and destination with ETL's core replication logic\n    info!(\"Creating ETL pipeline with custom store and HTTP destination\");\n    let mut pipeline = Pipeline::new(pipeline_config, store, destination);\n\n    // Step 5: Start the pipeline\n    // This will:\n    // 1. Load any existing state from your custom store\n    // 2. Connect to Postgres and start listening for changes\n    // 3. Begin replicating data through your custom destination\n    info!(\"Starting pipeline - this will connect to Postgres and begin replication\");\n    pipeline.start().await?;\n\n    // For demo purposes, let it run for 30 seconds then gracefully shut down\n    info!(\"Pipeline running! Watch the logs to see your custom components in action.\");\n    info!(\"Will run for 30 seconds then shut down gracefully...\");\n\n    tokio::time::sleep(Duration::from_secs(30)).await;\n\n    info!(\"Shutting down pipeline gracefully...\");\n    // pipeline.shutdown().await?;  // Uncomment if available in your ETL version\n\n    // In production, you'd typically call:\n    // pipeline.wait().await?;  // This blocks forever until manual shutdown\n\n    info!(\"=== ETL Pipeline Demo Complete ===\");\n    Ok(())\n}\n</code></pre> <p>Result: Running <code>cargo run</code> should now start your pipeline and show detailed logs from your custom components.</p>"},{"location":"tutorials/custom-implementations/#step-6-test-your-implementation","title":"Step 6: Test Your Implementation","text":"<p>Verify your custom components work correctly:</p> <pre><code># Check that everything compiles\ncargo check\n</code></pre> <p>Result: Should see \"Finished dev [unoptimized + debuginfo] target(s)\"</p> <pre><code># Run the pipeline (will fail without Postgres setup, but shows component initialization)\ncargo run\n</code></pre> <p>Result: You should see logs from your custom store being created and HTTP destination being configured.</p>"},{"location":"tutorials/custom-implementations/#checkpoint-what-youve-built","title":"Checkpoint: What You've Built","text":"<p>You now have working custom ETL components:</p> <p>\u2705 Custom Store: Implements dual-layer caching with detailed logging \u2705 HTTP Destination: Sends data via HTTP with automatic retry logic \u2705 Complete Pipeline: Integrates both components with ETL's core engine \u2705 Proper Error Handling: Follows ETL's error patterns and logging</p>"},{"location":"tutorials/custom-implementations/#key-patterns-youve-mastered","title":"Key Patterns You've Mastered","text":"<p>Store Architecture:</p> <ul> <li>Cache-first reads for performance</li> <li>Dual-write pattern for data consistency</li> <li>Startup loading from persistent storage</li> <li>Thread-safe concurrent access with Arc/Mutex</li> </ul> <p>Destination Patterns:</p> <ul> <li>Exponential backoff retry logic</li> <li>Smart error classification (retry 5xx, fail 4xx)</li> <li>Efficient batching and empty batch handling</li> <li>Clean data transformation from ETL to API formats</li> </ul>"},{"location":"tutorials/custom-implementations/#next-steps","title":"Next Steps","text":"<ul> <li>Connect to real Postgres \u2192 Configure Postgres for Replication</li> <li>Understand the architecture \u2192 ETL Architecture</li> <li>Contribute to ETL \u2192 Open an issue with your custom implementations</li> </ul>"},{"location":"tutorials/custom-implementations/#see-also","title":"See Also","text":"<ul> <li>ETL Architecture - Understanding the system design</li> <li>API Reference - Complete trait documentation</li> <li>Build your first pipeline - Start with the basics if you haven't yet</li> </ul>"},{"location":"tutorials/first-pipeline/","title":"Build Your First ETL Pipeline","text":"<p>Learn the fundamentals by building a working pipeline in 15 minutes</p> <p>By the end of this tutorial, you'll have a complete ETL pipeline that streams data changes from Postgres to a memory destination in real-time. You'll see how to set up publications, configure pipelines, and handle live data replication.</p>"},{"location":"tutorials/first-pipeline/#what-youll-build","title":"What You'll Build","text":"<p>A real-time data pipeline that:</p> <ul> <li>Monitors a Postgres table for changes</li> <li>Streams INSERT, UPDATE, and DELETE operations</li> <li>Stores replicated data in memory for immediate access</li> </ul>"},{"location":"tutorials/first-pipeline/#who-this-tutorial-is-for","title":"Who This Tutorial Is For","text":"<ul> <li>Rust developers new to ETL</li> <li>Anyone interested in Postgres logical replication</li> <li>Developers building data synchronization tools</li> </ul> <p>Time required: 15 minutes Difficulty: Beginner</p>"},{"location":"tutorials/first-pipeline/#safety-note","title":"Safety Note","text":"<p>This tutorial uses an isolated test database. To clean up, simply drop the test database when finished. No production data is affected.</p>"},{"location":"tutorials/first-pipeline/#step-1-set-up-your-environment","title":"Step 1: Set Up Your Environment","text":"<p>Create a new Rust project for this tutorial:</p> <pre><code>cargo new etl-tutorial\ncd etl-tutorial\n</code></pre> <p>Add ETL to your dependencies in <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\netl = { git = \"https://github.com/supabase/etl\" }\ntokio = { version = \"1.0\", features = [\"full\"] }\n</code></pre> <p>Checkpoint: Run <code>cargo check</code> - it should compile successfully.</p>"},{"location":"tutorials/first-pipeline/#step-2-prepare-postgres","title":"Step 2: Prepare Postgres","text":"<p>Connect to your Postgres server and create a test database:</p> <pre><code>CREATE DATABASE etl_tutorial;\n\\c etl_tutorial\n\n-- Create a sample table\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Insert sample data\nINSERT INTO users (name, email) VALUES\n    ('Alice Johnson', 'alice@example.com'),\n    ('Bob Smith', 'bob@example.com');\n</code></pre> <p>Create a publication for replication:</p> <pre><code>CREATE PUBLICATION my_publication FOR TABLE users;\n</code></pre> <p>Checkpoint: Verify the publication exists:</p> <pre><code>SELECT * FROM pg_publication WHERE pubname = 'my_publication';\n</code></pre> <p>You should see one row returned.</p>"},{"location":"tutorials/first-pipeline/#step-3-configure-your-pipeline","title":"Step 3: Configure Your Pipeline","text":"<p>Replace the contents of <code>src/main.rs</code>:</p> <pre><code>use etl::config::{BatchConfig, PgConnectionConfig, PipelineConfig, TlsConfig};\nuse etl::pipeline::Pipeline;\nuse etl::destination::memory::MemoryDestination;\nuse etl::store::both::memory::MemoryStore;\nuse std::error::Error;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    // Configure Postgres connection.\n    let pg_connection_config = PgConnectionConfig {\n        host: \"localhost\".to_string(),\n        port: 5432,\n        name: \"postgres\".to_string(),\n        username: \"postgres\".to_string(),\n        password: Some(\"your_password\".to_string().into()),\n        tls: TlsConfig {\n            trusted_root_certs: String::new(),\n            enabled: false,\n        },\n    };\n\n    // Configure pipeline behavior.\n    let pipeline_config = PipelineConfig {\n        id: 1,\n        publication_name: \"my_publication\".to_string(),\n        pg_connection: pg_connection_config,\n        batch: BatchConfig {\n            max_size: 1000,\n            max_fill_ms: 5000,\n        },\n        table_error_retry_delay_ms: 10000,\n        max_table_sync_workers: 4,\n    };\n\n    // Create stores and destination.\n    let store = MemoryStore::new();\n    let destination = MemoryDestination::new();\n\n    // We spawn a task to periodically print the content of the destination.\n    let destination_clone = destination.clone();\n    tokio::spawn(async move {\n        loop {\n            println!(\"Destination Contents At This Time\\n\");\n\n            // Table rows are the initial rows in the table that are copied.\n            for (table_id, table_rows) in destination_clone.table_rows().await {\n                println!(\"Table ({:?}): {:?}\", table_id, table_rows);\n            }\n\n            // Events are realtime events that are sent by Postgres after the table has been copied.\n            for event in destination_clone.events().await {\n                println!(\"Event: {:?}\", event);\n            }\n\n            tokio::time::sleep(std::time::Duration::from_secs(1)).await;\n\n            print!(\"\\n\\n\");\n        }\n    });\n\n    println!(\"Starting ETL pipeline...\");\n\n    // Create and start the pipeline.\n    let mut pipeline = Pipeline::new(pipeline_config, store, destination);\n    pipeline.start().await?;\n\n    println!(\"Waiting for pipeline to finish...\");\n\n    // Wait for the pipeline to finish, without a shutdown signal it will continue to work until the\n    // connection is closed.\n    pipeline.wait().await?;\n\n    Ok(())\n}\n</code></pre> <p>Important: Replace <code>\"your_password\"</code> with your Postgres password.</p>"},{"location":"tutorials/first-pipeline/#step-4-start-your-pipeline","title":"Step 4: Start Your Pipeline","text":"<p>Run your pipeline:</p> <pre><code>cargo run\n</code></pre> <p>You should see output like:</p> <pre><code>Starting ETL pipeline...\nWaiting for pipeline to finish...\n\nDestination Contents At This Time\n\nDestination Contents At This Time\n\nTable (TableId(32341)): [TableRow { values: [I32(1), String(\"Alice\"), String(\"alice@example.com\"), TimeStampTz(2025-08-05T11:14:54.400235Z)] }, TableRow { values: [I32(2), String(\"Bob\"), String(\"bob@example.com\"), TimeStampTz(2025-08-05T11:14:54.400235Z)] }, TableRow { values: [I32(3), String(\"Charlie\"), String(\"charlie@example.com\"), TimeStampTz(2025-08-05T11:14:54.400235Z)] }]\nTable (TableId(245615)): [TableRow { values: [I32(1), Array(I32([Some(1), Some(2), None, Some(4)]))] }, TableRow { values: [I32(2), Array(I32([None, None, Some(3)]))] }, TableRow { values: [I32(3), Array(I32([Some(5), None]))] }, TableRow { values: [I32(4), Array(I32([None]))] }, TableRow { values: [I32(5), Null] }]\n</code></pre> <p>Checkpoint: Your pipeline is now running and has completed initial synchronization.</p>"},{"location":"tutorials/first-pipeline/#step-5-test-real-time-replication","title":"Step 5: Test Real-Time Replication","text":"<p>With your pipeline running, open a new terminal and connect to Postgres:</p> <pre><code>psql -d etl_tutorial\n</code></pre> <p>Make some changes to test replication:</p> <pre><code>-- Insert a new user\nINSERT INTO users (name, email) VALUES ('Charlie Brown', 'charlie@example.com');\n\n-- Update an existing user\nUPDATE users SET name = 'Alice Cooper' WHERE email = 'alice@example.com';\n\n-- Delete a user\nDELETE FROM users WHERE email = 'bob@example.com';\n</code></pre> <p>Checkpoint: In your pipeline terminal, you should see log messages indicating these changes were captured and processed.</p>"},{"location":"tutorials/first-pipeline/#step-6-verify-data-replication","title":"Step 6: Verify Data Replication","text":"<p>The data is now replicated in your memory destination. While this tutorial uses memory (perfect for testing), the same pattern works with BigQuery, DuckDB, or custom destinations.</p> <p>Checkpoint: You've successfully built and tested a complete ETL pipeline!</p>"},{"location":"tutorials/first-pipeline/#what-youve-learned","title":"What You've Learned","text":"<p>You've mastered the core ETL concepts:</p> <ul> <li>Publications define which tables to replicate</li> <li>Pipeline configuration controls behavior and performance</li> <li>Memory destinations provide fast, local testing</li> <li>Real-time replication captures all data changes automatically</li> </ul>"},{"location":"tutorials/first-pipeline/#cleanup","title":"Cleanup","text":"<p>Remove the test database:</p> <pre><code>DROP DATABASE etl_tutorial;\n</code></pre>"},{"location":"tutorials/first-pipeline/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics:</p> <ul> <li>Build custom implementations \u2192 Custom Stores and Destinations</li> <li>Configure Postgres properly \u2192 Configure Postgres for Replication</li> <li>Understand the architecture \u2192 ETL Architecture</li> </ul>"},{"location":"tutorials/first-pipeline/#see-also","title":"See Also","text":"<ul> <li>Custom Implementation Tutorial - Advanced patterns</li> <li>API Reference - Complete configuration options</li> <li>ETL Architecture - Understand the design</li> </ul>"}]}